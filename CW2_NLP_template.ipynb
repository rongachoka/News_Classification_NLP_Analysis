{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CW2: Natural Language Processing: Classification of News Articles\n",
    "This coursework extends Lab 8, using the content of news articles to classify them into one of the 4 following categories: `World, Sports, Business, Sci/Tech`.\n",
    "\n",
    "The original data source is http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html. You do not need to download the data from those websites, it has been made available on GCU learn in the compressed file 'news_dataset.zip', you simply need to download and extract it. You'll get files *train.csv* and *test.csv*. Place them in the same folder as this notebook (or change the path in `pd.read_csv()` accordingly).\n",
    "\n",
    "Import Conda environmet `lab8_NLP_news.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import (\n",
    "    TextVectorization,\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    Bidirectional,\n",
    "    Dense,\n",
    "    Embedding,\n",
    ")\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preparation\n",
    "Import data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset. As done in lab 4, you usually have a look at the data. Here we simply show the first 5 rows\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "print(train_data.head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change labels from 1-4 to 0-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substract 1 from the target variable for both, train and test data\n",
    "train_data[\"Class Index\"] = train_data[\"Class Index\"] - 1\n",
    "test_data[\"Class Index\"] = test_data[\"Class Index\"] - 1\n",
    "label_names = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"] # 0 is \"World\", 1 is \"Sports\", ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "\n",
    "Lowercase descriptions and clean up non-letter characters.\n",
    "\n",
    "**TODO 1**: Remove the occurences of \"http\" \"href\", \"https\" and \"www\". Also, remove the names of the news agencies (Reuters, AP, ...) from the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the data\n",
    "def preprocess_text(string):\n",
    "    # TODO 1: REMOVE http (html, ...) and news agencies names (Reuters, AP, ...)\n",
    "\n",
    "\n",
    "    string = string.lower()\n",
    "    string = string.replace(\"'\", \" \")\n",
    "    string = string.replace(\"\\\\\", \" \")\n",
    "    string = re.sub(r\"[^a-zA-Z]\", \" \", string)\n",
    "    return string\n",
    "train_data[\"Description\"] = train_data[\"Description\"].apply(preprocess_text)\n",
    "test_data[\"Description\"] = test_data[\"Description\"].apply(preprocess_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dataframes to lists (needed for further functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_list = train_data[\"Description\"].tolist()\n",
    "test_data_list = test_data[\"Description\"].tolist()\n",
    "train_labels_list = train_data[\"Class Index\"].tolist()\n",
    "test_labels_list = test_data[\"Class Index\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create validation set (taking out 20% of test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data_list, validation_data_list,\n",
    " train_labels_list, validation_labels_list) = train_test_split(train_data_list, train_labels_list, test_size=0.2, stratify=train_labels_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "Remove stopwords from the *stopword* corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "def remove_stopwords(data_list):\n",
    "    stopword_list = stopwords.words(\"english\")\n",
    "    for i in range(len(data_list)):\n",
    "        data_list[i] = \" \".join(\n",
    "            [word for word in data_list[i].split() if word not in (stopword_list)]\n",
    "        )\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_list = remove_stopwords(train_data_list)\n",
    "validation_data_list = remove_stopwords(validation_data_list)\n",
    "test_data_list = remove_stopwords(test_data_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: STEMMING\n",
    "Use **Stemming** or **Lemmatization** to grammatical word variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: YOUR CODE HERE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recurrent Neural Network classifier with Keras and Tensorflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode the labels (0 -> 0001, 1 -> 0010, ...) to match NN classifier head output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the labels are of value 0, 1, 2, 3. We need to convert them to one-hot encoded vectors \n",
    "train_labels = tensorflow.keras.utils.to_categorical(np.array(train_labels_list), num_classes=4)\n",
    "validation_labels = tensorflow.keras.utils.to_categorical(np.array(validation_labels_list), num_classes=4)\n",
    "test_labels = tensorflow.keras.utils.to_categorical(np.array(test_labels_list), num_classes=4)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we use the TextVectorization layer to convert the text to a sequence of integers. You can read about how this works  [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will create a vocabulary of the top 2500 words and then convert the text to a sequence of numbers\n",
    "vocab_size = 2500 # how many of the most frequent words to keep\n",
    "sequence_length = 30 # how many words we use to represent a description\n",
    "vectorizer = TextVectorization(max_tokens=vocab_size, output_sequence_length=sequence_length)\n",
    "# fit the vectorizer on the training data (find the most common words and assign values to them)\n",
    "vectorizer.adapt(train_data_list)\n",
    "# show the selected words\n",
    "print(vectorizer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the training data (replace words by their associated values)\n",
    "train_data_vectorized = vectorizer(train_data_list)\n",
    "# vectorize the validation data\n",
    "validation_data_vectorized = vectorizer(validation_data_list)\n",
    "# vectorize the test data\n",
    "test_data_vectorized = vectorizer(test_data_list)\n",
    "# this shape shows that we have our 96000 training examples, each as a vector of 30 integers\n",
    "print(train_data_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how a vectorized news description looks like\n",
    "print(train_data_vectorized[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build a RNN.\n",
    "\n",
    "We use an *Embedding layer* to convert the integer sequences to embeddings. You can read about how this works [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding).\n",
    "\n",
    "The Bidirectional layer takes the input and passes it forwards and backwards through the LSTM (Long-Short Term Memory) layer. It allows the network to learn the context of the sentence in both directions, so information from both previous and following words is taken into account.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 32))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(4, activation=\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# fit the model\n",
    "model.fit(\n",
    "    train_data_vectorized,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    verbose=1,\n",
    "    validation_data=(validation_data_vectorized, validation_labels),\n",
    ")\n",
    "\n",
    "# predict the labels on the test data\n",
    "rnn_predictions = model.predict(test_data_vectorized)\n",
    "# since predictions are one-hot encoded, we convert them to an int label taking the output with higher value\n",
    "rnn_predictions = np.argmax(rnn_predictions, axis=1)\n",
    "\n",
    "# calculate the accuracy score\n",
    "accuracy = accuracy_score(test_labels_list, rnn_predictions)\n",
    "print(\"Test Set Accuracy: \", accuracy)\n",
    "# create a confusion matrix\n",
    "cm = confusion_matrix(test_labels_list, rnn_predictions)\n",
    "# plot the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=label_names, yticklabels=label_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO 3: Creating Example Predictions\n",
    "Test the model. Create some fresh sample news (copy them from a news site like BBC) then see if the model can predict the correct labels. Use one example per class.\n",
    "\n",
    "Also, find one example that gets misclassified and briefly discuss here why it might happen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_news = list()\n",
    "# TODO 3: create some fresh sample news (copy them from a news site like BBC) then see if the model can predict the correct labels\n",
    "# Use one example per class\n",
    "sample_news.append(\"Paste here a recent example of world news\")\n",
    "sample_news.append(\"Paste here a recent example of sports news\")\n",
    "sample_news.append(\"Paste here a recent example of business news\")\n",
    "sample_news.append(\"Paste here a recent example of sci/tech news\")\n",
    "# Also, find one example that gets misclassified and discuss why it might happen in the markdown above.\n",
    "sample_news.append(\"Paste here a recent example of news that get misclassified!\")\n",
    "\n",
    "# preprocess the sample news\n",
    "sample_news = [preprocess_text(i) for i in sample_news]\n",
    "sample_news = remove_stopwords(sample_news)\n",
    "\n",
    "# If you added stemming/lemmatization, do it on sample_news as well\n",
    "# your code here\n",
    "\n",
    "# vectorize the sample news\n",
    "sample_news_vectorized = vectorizer(sample_news)\n",
    "\n",
    "# predict using the neural network\n",
    "prediction = model.predict(sample_news_vectorized)\n",
    "prediction = np.argmax(prediction, axis=1)\n",
    "for i in prediction:\n",
    "     print(\"NN classifier prediction: \", label_names[prediction[i]], \"News\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 4: Create a new model\n",
    "Try to improve the RNN model, of implement a CNN (with 1D convolution layers), that beats the baseline RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4: YOUR CODE HERE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "This cell goes to the very bottom of your submitted notebok.\n",
    "You are requried to link the sources and web-links that you have used for various parts of this coursework. \n",
    "\n",
    "Write them sources used in the following format similar to the first examle in the sources list below :\n",
    "\n",
    "    - what you have used them for : web-link\n",
    "\n",
    "Sources:\n",
    "\n",
    "- Implement a recurrent neural network : https://peterroelants.github.io/posts/rnn-implementation-part01/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('AIML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "977335c72b126e3991b9de8b6fc74c7a8bf9097191ab51ecd2769bb8eacdf950"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
